<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Face Mesh Detection</title>

    <style>
        body {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            background: #111;
        }
        canvas, video {
            position: absolute;
            transform: scaleX(-1); /* effet miroir */
        }
    </style>
</head>
<body>

<video id="video" autoplay playsinline width="640" height="480"></video>
<canvas id="output" width="640" height="480"></canvas>

<!-- ✅ TFJS -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.14.0/dist/tf.min.js"></script>

<!-- ✅ Dépendances Mediapipe -->
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>

<!-- ✅ Modèle TFJS Face Landmarks -->
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@1.0.6/dist/face-landmarks-detection.min.js"></script>

<script>
(async () => {
    const video = document.getElementById("video");
    const canvas = document.getElementById("output");
    const ctx = canvas.getContext("2d");

    // ✅ Démarrer la webcam
    async function startCamera() {
        const stream = await navigator.mediaDevices.getUserMedia({
            video: { width: 640, height: 480 },
            audio: false,
        });
        video.srcObject = stream;
        return new Promise(resolve => { video.onloadedmetadata = () => resolve(); });
    }

    await startCamera();

    // ✅ Charger le modèle FaceMesh via TensorFlow JS
    const model = await faceLandmarksDetection.load(
        faceLandmarksDetection.SupportedPackages.mediapipeFaceMesh,
        {
            maxFaces: 1,
            shouldLoadIrisModel: true,
        }
    );

    console.log("✅ FaceMesh chargé !");

    // ✅ Loop d’analyse
    async function detect() {
        const predictions = await model.estimateFaces({
            input: video,
            flipHorizontal: true
        });

        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

        if (predictions.length > 0) {
            predictions.forEach(face => {
                const keypoints = face.scaledMesh;

                ctx.fillStyle = "cyan";
                keypoints.forEach(([x, y]) => {
                    ctx.beginPath();
                    ctx.arc(x, y, 1.5, 0, 2 * Math.PI);
                    ctx.fill();
                });
            });
        }

        requestAnimationFrame(detect);
    }

    detect();
})();
</script>

</body>
</html>
